<div align="center">
  <h1>Glide</h1>
  
  <p>
    <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python" />
    <img src="https://img.shields.io/badge/TypeScript-007ACC?style=for-the-badge&logo=typescript&logoColor=white" alt="TypeScript" />
    <img src="https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB" alt="React" />
    <img src="https://img.shields.io/badge/Next.js-000000?style=for-the-badge&logo=next.js&logoColor=white" alt="Next.js" />
    <img src="https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&logo=fastapi&logoColor=white" alt="FastAPI" />
    <img src="https://img.shields.io/badge/Tailwind_CSS-38B2AC?style=for-the-badge&logo=tailwind-css&logoColor=white" alt="Tailwind CSS" />
  </p>

  <p>A sophisticated, AI-powered desktop control system powered by MediaPipe and React.</p>
</div>

Glide combines real-time computer vision with a modern, glassmorphism-styled dashboard for a seamless and futuristic user experience.

## âœ¨ Features

### ğŸ¤– Computer Vision and AI
- **Real-Time Hand Tracking**: High-fidelity landmark detection using **MediaPipe**.
- **Customizable Gesture Recognition**: Custom Random Forest classifier (Scikit-learn) for high-accuracy gesture identification.
- **Data Collection Engine**: Capture landmark samples in real-time to train new gestures.
- **Instant Retraining**: Retrain the AI model directly from the UI to incorporate new gestures on-the-fly.

### ğŸ–±ï¸ Dynamic Desktop Control
- **Precision Cursor Control**: Smooth mouse movement using vector projection from hand landmarks to minimize jitter.
- **Gesture-Based Clicking**:
    - **Left Click**: Short pinch gesture.
    - **Double Click**: Two rapid short pinches.
    - **Right Click**: Long hold pinch gesture.
- **Global Action Mapping**: Map specific gestures to a wide range of system actions.

### ğŸ“š Comprehensive Action Library
- **Media Controls**: Play/Pause, Next Track, Previous Track, Mute.
- **Audio Management**: Volume Up, Volume Down.
- **Display Settings**: Brightness Up, Brightness Down.
- **Navigation**: Scroll Up, Scroll Down, Switch Tab (Alt+Tab / Cmd+Tab).
- **System Utilities**: Screenshot capture, Minimize Window.

### ğŸ“Š Professional Dashboard
- **Live Camera Feed**: Low-latency video stream with real-time hand landmark overlays.
- **Real-Time Statistics**: Monitor FPS, total detections, actions executed, and model confidence scores.
- **System Status**: Instant feedback on camera status, model readiness, and WebSocket connectivity.
- **Gesture Management**: Create, edit, and delete gestures and their assigned actions.

### âš¡ Communication and Scalability
- **WebSocket Integration**: Full-duplex communication between the Python backend and React frontend for instantaneous feedback.
- **Cross-Platform Support**: Optimized hotkeys and system controls for both macOS and Windows.

## ğŸ“‚ Project Structure

```text
Glide/
â”œâ”€â”€ engine/                     # AI & System Control Logic
â”‚   â”œâ”€â”€ data/                   # Saved landmark samples (npy format)
â”‚   â”œâ”€â”€ models/                 # Serialized model weights and metadata
â”‚   â”œâ”€â”€ main.py                 # FastAPI server, WebSocket orchestrator
â”‚   â”œâ”€â”€ gesture_detector.py      # MediaPipe integration for landmark extraction
â”‚   â”œâ”€â”€ gesture_classifier.py    # Random Forest training and prediction logic
â”‚   â”œâ”€â”€ mouse_controller.py      # Cursor movement and click detection logic
â”‚   â”œâ”€â”€ desktop_controller.py    # Platform-specific automation (PyAutoGUI)
â”‚   â”œâ”€â”€ action_mapper.py        # Mapping interface for system actions
â”‚   â”œâ”€â”€ websocket_manager.py     # Connection management for real-time updates
â”‚   â”œâ”€â”€ gestures.json           # Persistent storage for mappings
â”‚   â”œâ”€â”€ Dockerfile              # Docker configuration for engine
â”‚   â””â”€â”€ requirements.txt        # Python dependency specifications
â”œâ”€â”€ app/                        # Next.js App Router and page layouts
â”œâ”€â”€ components/                 # Reusable UI components (LiveFeed, UI, Dashboard)
â”œâ”€â”€ hooks/                      # Custom React hooks (useMobile, etc.)
â”œâ”€â”€ lib/                        # Utility functions and type definitions
â”œâ”€â”€ public/                     # Static assets and icons
â”œâ”€â”€ styles/                     # Tailwind CSS and global themes
â”œâ”€â”€ package.json               # Node.js dependency specifications
â”œâ”€â”€ Dockerfile.web             # Docker configuration for frontend
â”œâ”€â”€ docker-compose.yml         # Container orchestration
â”œâ”€â”€ install.sh                 # Unified installation script
â”œâ”€â”€ .gitignore                  # Consolidated repository ignore rules
â””â”€â”€ readme.md                  # Project documentation
```

## ğŸ› ï¸ Setup and Installation

You can run Glide using **Docker** (Recommended) or a **Local Installation**.

### Option 1: ğŸ³ Docker (Recommended)
Run the entire stack in isolated containers. This ensures compatibility with any Python version.

```bash
# Start backend and frontend
docker-compose up --build
```
- **Dashboard**: http://localhost:3000
- **Engine API**: http://localhost:8053

### Option 2: âš¡ Quick Install Script
Use the provided script to set up everything automatically on your local machine (Brew must be installed)

```bash
chmod +x install.sh
./install.sh
```

### Option 3: ğŸ”§ Manual Installation

#### ğŸ“‹ Prerequisites
- Python 3.9 - 3.11 (Python 3.13 not supported by MediaPipe yet)
- Node.js 18 or higher (pnpm recommended)
- Integrated or external webcam

#### 1. âš™ï¸ Engine Configuration (Backend)
Navigate to the engine directory and initialize the environment:

```bash
cd engine
# Use Python 3.11 if available for best compatibility
python3.11 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

Launch the engine server:
```bash
python3 main.py
```
Default endpoint: http://localhost:8053

#### 2. ğŸ–¥ï¸ Dashboard Configuration (Frontend)
From the root directory, install dependencies and start the UI:

```bash
pnpm install
# Alternative: npm install
```

Start the development server:
```bash
pnpm dev
# Alternative: npm run dev
```
Dashboard address: http://localhost:3000

## ğŸš€ Usage Guide

1. **Initialize**: Ensure the backend and frontend are running. Open the dashboard in your browser.
2. **Connectivity**: Verify that Camera, Model, and WebSocket indicators are active.
3. **Gesture Creation**:
   - Navigate to the Gestures tab.
   - Click Add Gesture, provide a name, and select a system action.
   - Record landmark samples by following the on-screen prompts.
4. **Retrain**: Click Retrain Model to synchronize the AI classifier with your new data.
5. **Control**: Toggle Start Detection and use your hand gestures to interact with your system.

## ğŸ—ï¸ Technical Stack

### ğŸ Backend
- **FastAPI**: High-performance asynchronous API framework.
- **MediaPipe**: Google's modular ML framework for computer vision.
- **Scikit-learn**: Machine learning library for the Random Forest classifier.
- **OpenCV**: Real-time image processing.
- **PyAutoGUI**: Cross-platform GUI automation.

### âš›ï¸ Frontend
- **Next.js**: React framework for performance and routing.
- **Tailwind CSS**: Utility-first styling for the glassmorphism design.
- **Framer Motion**: Production-ready motion library for animations.
- **Recharts**: Composable charting library for data visualization.
- **Lucide React**: Clean and consistent icon library.

## ğŸ“œ License
This project is licensed under the MIT License.
